The scripts in this folder query the Elasticsearch database used by
Domeo to retrieve annotations created by specific users. This code is
a work-around until we can build and test Connector plugins that work
with the Annotopia framework: https://github.com/Annotopia

This code uses the official Python interface to Elastic search
(https://github.com/elasticsearch/elasticsearch-py/releases). Complete
documentation for the API is here:
http://elasticsearch-py.readthedocs.org/en/master/api.html

------------------------------------------------------------
Pre-requisites:
------------------------------------------------------------

* rdflib-4.1.2 and rdflib_jsonld-0.2 

These can be found at the rdflib site and
https://pypi.python.org/pypi/rdflib-jsonld/0.2-dev respectively. They
are necessary to use rdflib's Graph classh's general function for
converting JSON-LD to RDF.

* the appropriate version of Elasticsearch.py:

1) Install the appropriate version of Elasticsearch.py (for Domeo on
Ubuntu running Elastic search 0.90.1 use
https://github.com/elasticsearch/elasticsearch-py/archive/0.4.5.tar.gz)

2) unpack and install

python setup.py build
sudo python setup.py install

3) Mysql DB config file - Domeo-DB-config.txt

USERNAME=<USERNAME>;
PASSWORD=<PASSWORD>;

----------------------------------------------------------------
Query Elasticsearch document store in order to export RDF graph
----------------------------------------------------------------

See the convertDDIJsonLDToRDF.py usage by running it with no commandline paramaters. Currently this outputs: 

"convertDDIJsonLDToRDF <query string> <collection> <output file name> <old version>(1: want old versions, 0: only get last version) <verbose>(optional 1=True, 0=False (default)"

Example - query for all annotations belonging to a Domeo user with ID "80808080466465c00146674fe5190024" and write them to RDF in a file called DDI-expert1.xml:

$ python convertDDIJsonLDToRDF.py 80808080466465c00146674fe5190024 devb30 domeo-ddi-annotations-in-rdf-07092014.xml 0

NOTE: results are limited to 10,000 at this time.

NOTE: empty strings in the JSON-LD provided by Elasticsearch are
incorrectly converted to blank nodes by rdflib-jsonld. To address this
issue, the script converts empty strings occurring in the body
component to "<empty>" 

NOTE: Annotations stored by Domeo use "!DOMEO_NS!" as string
placeholder for ":". This is is a leftover from the first days of
JSON-LD where the Domeo architect (Paulo Ciccarese) did not have a
working context in place. For now, the script replaces these strings
before rdflib-jsonld converts the data to RDF.

NOTE: Elasticsearch has multiple sub-graphs under 'domeo' labeled by
'type'. The scripts in this folder query for sets from type 'devb30'
which appears to be the complete graphs for each annotation. The other
types appear to be used for tracking changes. 


------------------------------------------------------------
Load Graph to Virtuoso endpoint for query   
------------------------------------------------------------

Initial load:
$ isql-vt
SQL> ld_dir ('/path/to/files', '*.n3', '<graph name>');
SQL> rdf_loader_run();
SQL> select * from DB.DBA.load_list;

Reloading (DELETE THE OLD GRAPH FIRST from Virtuoso composer!):
$ isql-vt
SQL> log_enable(3,1); # see http://www.openlinksw.com/dataspace/dav/wiki/Main/VirtTipsAndTricksGuideDeleteLargeGraphs

SQL> SPARQL CLEAR GRAPH <http://dbmi-icode-01.dbmi.pitt.edu/linkedSPLs/>;
SQL> update DB.DBA.load_list set ll_state = 0 where ll_file = '<name of RDF file>';
SQL> rdf_loader_run();
SQL> select * from DB.DBA.load_list; 


------------------------------------------------------------
Query to export csv
------------------------------------------------------------

1) load the RDF output into an RDF endpoint (e.g., Virtuoso). Give the
graph a very specific name such as "<http://purl.org/net/nlprepository/changed-ddi-expert1-annotation"

2) The queries in example-queries-NLP-DDI.txt show how to query DDI
and Highlight annotations done for a specific source. Note that you
might need to adjust the graph in the FROM part of the query and the
URL in the oa:hasSource predicate. 

3) Save qurey results as csv and follow instructions in PKDDISets/README for post-process data set  


------------------------------------------------------------
Clean csv dataset
------------------------------------------------------------
post process csv from sparql query

replace "xsd:String" to ""
replace "http://dbmi-icode-01.dbmi.pitt.edu:2020/vocab/resource/" to "dikbD2R:"
replace "nodeID://b[0-9]+" to ""